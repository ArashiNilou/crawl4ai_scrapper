import asyncio
import time
import re
import json
import sys

from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode, BrowserConfig
from dotenv import load_dotenv

from config import BASE_URL, REQUIRED_KEYS, SCROLL_CONFIG
from utils.data_utils import save_exposants_to_csv
from utils.scraper_utils import (
    get_browser_config,
    get_css_extraction_strategy,
    get_llm_strategy,
    extract_all_exposants,
    get_total_exposants_count
)

load_dotenv()


async def crawl_exposants_infinite_scroll():
    """
    Fonction principale pour crawler les exposants avec scroll infini.
    Version adapt√©e pour Crawl4AI 0.6
    """
    print("=== D√©marrage du crawler d'exposants avec scroll infini ===")
    print(f"URL cible: {BASE_URL}")
    
    # Configuration
    browser_config = get_browser_config()
    
    # Strat√©gie d'extraction (CSS par d√©faut, LLM en fallback)
    try:
        css_strategy = get_css_extraction_strategy()
        llm_strategy = get_llm_strategy()
    except Exception as e:
        print(f"Erreur lors de la configuration des strat√©gies: {e}")
        return
    
    session_id = "exposant_infinite_scroll_session"
    seen_names = set()
    
    start_time = time.time()
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        try:
            # √âtape 1: Obtenir une estimation du nombre total
            print("√âtape 1: Estimation du nombre total d'exposants...")
            total_estimate = await get_total_exposants_count(crawler, BASE_URL, session_id)
            print(f"Estimation: ~{total_estimate} exposants d√©tect√©s")
            
            # √âtape 2: Extraction avec CSS
            print("\n√âtape 2: Extraction avec strat√©gie CSS...")
            exposants_css = await extract_all_exposants(
                crawler, BASE_URL, session_id, css_strategy, 
                REQUIRED_KEYS, seen_names.copy()
            )
            
            # √âtape 3: Extraction avec LLM si CSS n'a pas bien fonctionn√©
            exposants_llm = []
            if len(exposants_css) < max(total_estimate * 0.3, 5):  # Si moins de 30% r√©cup√©r√©s ou moins de 5
                print(f"\n√âtape 3: Extraction avec strat√©gie LLM (fallback)...")
                print(f"CSS a r√©cup√©r√© {len(exposants_css)} exposants sur {total_estimate} estim√©s")
                try:
                    exposants_llm = await extract_all_exposants(
                        crawler, BASE_URL, session_id, llm_strategy, 
                        REQUIRED_KEYS, seen_names.copy()
                    )
                except Exception as e:
                    print(f"Erreur avec strat√©gie LLM: {e}")
                    exposants_llm = []
            else:
                print(f"\n√âtape 3: Extraction CSS suffisante ({len(exposants_css)} exposants), LLM non n√©cessaire")
            
            # √âtape 4: Combiner les r√©sultats
            print("\n√âtape 4: Combinaison des r√©sultats...")
            all_exposants = combine_results(exposants_css, exposants_llm, seen_names)
            
        except Exception as e:
            print(f"Erreur lors du crawling: {e}")
            print(f"Type d'erreur: {type(e).__name__}")
            import traceback
            traceback.print_exc()
            return
    
    # √âtape 5: Sauvegarde et statistiques
    end_time = time.time()
    duration = end_time - start_time
    
    if all_exposants:
        filename = f"exposants_scroll_{int(time.time())}.csv"
        save_exposants_to_csv(all_exposants, filename)
        
        print(f"\n=== R√âSULTATS ===")
        print(f"‚úÖ {len(all_exposants)} exposants extraits et sauvegard√©s")
        print(f"üìÅ Fichier: {filename}")
        print(f"‚è±Ô∏è  Dur√©e: {duration:.1f} secondes")
        print(f"üìä Taux de r√©ussite: {len(all_exposants)}/{max(total_estimate, 1)} ({len(all_exposants)/max(total_estimate,1)*100:.1f}%)")
        
        # Statistiques d√©taill√©es
        print_statistics(all_exposants)
        
    else:
        print("‚ùå Aucun exposant n'a √©t√© trouv√©")
        print("\nüîß Suggestions de diagnostic:")
        print("1. V√©rifiez l'URL dans config.py")
        print("2. V√©rifiez les s√©lecteurs CSS dans config.py")
        print("3. Lancez 'python main.py test' pour analyser la structure HTML")
        print("4. V√©rifiez que le site est accessible et ne n√©cessite pas d'authentification")
    
    # Afficher les statistiques LLM si utilis√©
    if exposants_llm:
        try:
            if hasattr(llm_strategy, 'show_usage'):
                llm_strategy.show_usage()
        except Exception as e:
            print(f"Impossible d'afficher les stats LLM: {e}")


def combine_results(css_results: list, llm_results: list, seen_names: set) -> list:
    """
    Combine les r√©sultats des deux strat√©gies d'extraction.
    """
    print(f"Combinaison: {len(css_results)} (CSS) + {len(llm_results)} (LLM)")
    
    # Commencer avec les r√©sultats CSS
    combined = css_results.copy()
    seen_names.update(exp.get('nom_entreprise', '') for exp in css_results if exp.get('nom_entreprise'))
    
    # Ajouter les r√©sultats LLM uniques
    added_from_llm = 0
    for exp in llm_results:
        name = exp.get('nom_entreprise', '').strip()
        if name and name not in seen_names:
            combined.append(exp)
            seen_names.add(name)
            added_from_llm += 1
    
    print(f"R√©sultat final: {len(combined)} exposants ({added_from_llm} ajout√©s par LLM)")
    return combined


def print_statistics(exposants: list):
    """
    Affiche des statistiques d√©taill√©es sur les exposants extraits.
    """
    print(f"\n=== STATISTIQUES D√âTAILL√âES ===")
    
    # Statistiques par secteur
    secteurs = {}
    pays = {}
    villes = {}
    startups = {"oui": 0, "non": 0, "N/A": 0}
    
    for exp in exposants:
        # Secteurs
        secteur = exp.get('secteur_activite', 'N/A').strip()
        if secteur:
            secteurs[secteur] = secteurs.get(secteur, 0) + 1
        
        # Pays
        pays_exp = exp.get('pays', 'N/A').strip()
        if pays_exp:
            pays[pays_exp] = pays.get(pays_exp, 0) + 1
        
        # Villes
        ville = exp.get('ville', 'N/A').strip()
        if ville:
            villes[ville] = villes.get(ville, 0) + 1
        
        # Startups
        startup_status = exp.get('startup', 'N/A').lower().strip()
        if startup_status in ['oui', 'yes', 'true', '1']:
            startups["oui"] += 1
        elif startup_status in ['non', 'no', 'false', '0']:
            startups["non"] += 1
        else:
            startups["N/A"] += 1
    
    # Affichage des top 5
    print(f"üè¢ Top 5 secteurs:")
    for secteur, count in sorted(secteurs.items(), key=lambda x: x[1], reverse=True)[:5]:
        if secteur != 'N/A':
            print(f"  - {secteur}: {count}")
    
    print(f"\nüåç Top 5 pays:")
    for pays_name, count in sorted(pays.items(), key=lambda x: x[1], reverse=True)[:5]:
        if pays_name != 'N/A':
            print(f"  - {pays_name}: {count}")
    
    print(f"\nüèôÔ∏è Top 5 villes:")
    for ville_name, count in sorted(villes.items(), key=lambda x: x[1], reverse=True)[:5]:
        if ville_name != 'N/A':
            print(f"  - {ville_name}: {count}")
    
    print(f"\nüöÄ Statut startup:")
    for status, count in startups.items():
        if count > 0:
            print(f"  - {status.capitalize()}: {count}")


async def test_scroll_only():
    """
    Fonction de test pour v√©rifier l'acc√®s au site et analyser la structure.
    Version compl√®te pour Crawl4AI 0.6
    """
    print("=== TEST: Acc√®s au site et analyse du contenu ===")
    print(f"URL test√©e: {BASE_URL}")
    
    browser_config = get_browser_config()
    session_id = "test_session"
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        try:
            result = await crawler.arun(
                url=BASE_URL,
                config=CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS,
                    session_id=session_id,
                ),
            )
            
            if result.success and result.cleaned_html:
                html_content = result.cleaned_html
                print(f"‚úÖ Acc√®s r√©ussi, HTML r√©cup√©r√© ({len(html_content)} caract√®res)")
                
                # Sauvegarder pour inspection
                debug_filename = "debug_page.html"
                with open(debug_filename, "w", encoding="utf-8") as f:
                    f.write(html_content)
                print(f"üîç Page sauvegard√©e dans '{debug_filename}' pour inspection")
                
                # Analyser la structure
                analyze_html_structure(html_content)
                
                # Tester les s√©lecteurs CSS configur√©s
                print(f"\n=== TEST DES S√âLECTEURS CSS ===")
                test_css_selectors(html_content)
                
            else:
                print(f"‚ùå √âchec de l'acc√®s √† la page")
                if result.error_message:
                    print(f"Erreur: {result.error_message}")
                else:
                    print("Aucun contenu HTML r√©cup√©r√©")
                
        except Exception as e:
            print(f"‚ùå Erreur lors du test: {e}")
            import traceback
            traceback.print_exc()


def analyze_html_structure(html_content: str):
    """
    Analyse la structure HTML pour identifier les patterns d'√©l√©ments.
    """
    print(f"\n=== ANALYSE DE LA STRUCTURE HTML ===")
    
    # Patterns pour d√©tecter les √©l√©ments
    analysis_patterns = {
        "Divs avec class": r'<div[^>]+class="[^"]*"',
        "Spans avec class": r'<span[^>]+class="[^"]*"',
        "√âl√©ments avec 'company'": r'class="[^"]*company[^"]*"',
        "√âl√©ments avec 'card'": r'class="[^"]*card[^"]*"',
        "√âl√©ments avec 'exposant'": r'class="[^"]*exposant[^"]*"',
        "√âl√©ments avec 'partner'": r'class="[^"]*partner[^"]*"',
        "Attributs data-*": r'data-[^=]+="[^"]*"',
        "Titres H1-H6": r'<h[1-6][^>]*>([^<]+)</h[1-6]>',
        "Liens": r'<a[^>]+href="[^"]*"',
        "Images": r'<img[^>]+src="[^"]*"'
    }
    
    results = {}
    for name, pattern in analysis_patterns.items():
        matches = re.findall(pattern, html_content, re.IGNORECASE)
        results[name] = len(matches)
        
        if name == "Titres H1-H6" and matches:
            # Afficher quelques exemples de titres
            print(f"\nüìã Exemples de titres trouv√©s:")
            for i, title in enumerate(matches[:5]):
                clean_title = title.strip()
                if clean_title:
                    print(f"  {i+1}. {clean_title}")
    
    print(f"\nüìä √âl√©ments d√©tect√©s dans la page:")
    for name, count in results.items():
        if count > 0 and name != "Titres H1-H6":
            print(f"  - {name}: {count}")
    
    # Recherche de structures r√©p√©titives
    print(f"\nüîç Recherche de structures r√©p√©titives...")
    repetitive_classes = find_repetitive_classes(html_content)
    if repetitive_classes:
        print("Classes qui apparaissent fr√©quemment (potentielles cartes d'exposants):")
        for class_name, count in repetitive_classes[:10]:
            print(f"  - '{class_name}': {count} occurrences")


def find_repetitive_classes(html_content: str) -> list:
    """
    Trouve les classes CSS qui apparaissent fr√©quemment dans le HTML.
    """
    # Extraire toutes les classes
    class_pattern = r'class="([^"]*)"'
    all_classes = re.findall(class_pattern, html_content)
    
    # Compter les occurrences de chaque classe individuelle
    class_counts = {}
    for class_attr in all_classes:
        individual_classes = class_attr.split()
        for cls in individual_classes:
            if cls and len(cls) > 2:  # Ignorer les classes tr√®s courtes
                class_counts[cls] = class_counts.get(cls, 0) + 1
    
    # Retourner les classes les plus fr√©quentes (potentielles cartes)
    return sorted(class_counts.items(), key=lambda x: x[1], reverse=True)


def test_css_selectors(html_content: str):
    """
    Teste les s√©lecteurs CSS configur√©s sur le HTML r√©cup√©r√©.
    """
    from config import CSS_SELECTORS
    
    # Simulation simple des s√©lecteurs CSS avec regex
    # Note: Ce n'est pas parfait mais donne une id√©e
    
    selector_tests = {
        "secteur": CSS_SELECTORS.get("secteur", ""),
        "pays": CSS_SELECTORS.get("pays", ""),
        "ville": CSS_SELECTORS.get("ville", ""),
        "container": CSS_SELECTORS.get("container", "")
    }
    
    for name, selector in selector_tests.items():
        if selector:
            # Test basique avec regex (approximation)
            if "span" in selector and "p-0" in selector:
                pattern = r'<span[^>]*class="[^"]*p-0[^"]*"[^>]*>([^<]*)</span>'
            elif "div" in selector:
                pattern = r'<div[^>]*>([^<]*)</div>'
            elif "span" in selector:
                pattern = r'<span[^>]*>([^<]*)</span>'
            else:
                pattern = selector
            
            try:
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                print(f"  - {name} ('{selector}'): {len(matches)} √©l√©ments potentiels")
                
                # Afficher quelques exemples
                if matches:
                    for i, match in enumerate(matches[:3]):
                        clean_match = match.strip()
                        if clean_match:
                            print(f"    Ex{i+1}: {clean_match}")
            except re.error:
                print(f"  - {name}: S√©lecteur invalide pour test regex")


async def main():
    """
    Point d'entr√©e principal avec gestion des arguments.
    """
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == "test":
            await test_scroll_only()
        elif command == "analyze":
            await test_scroll_only()  # M√™me fonction pour l'analyse
        elif command == "help":
            print_help()
        else:
            print(f"‚ùå Commande inconnue: {command}")
            print_help()
    else:
        # Lancement normal du crawler
        await crawl_exposants_infinite_scroll()


def print_help():
    """
    Affiche l'aide sur les commandes disponibles.
    """
    print("=== AIDE - Crawler d'exposants ===")
    print("Commandes disponibles:")
    print("  python main.py          - Lance le crawl complet")
    print("  python main.py test     - Teste l'acc√®s et analyse la structure")
    print("  python main.py analyze  - Alias pour 'test'")
    print("  python main.py help     - Affiche cette aide")
    print("\nConfiguration:")
    print(f"  - URL cible: {BASE_URL}")
    print(f"  - Champs requis: {', '.join(REQUIRED_KEYS)}")
    print("\nFichiers de configuration:")
    print("  - config.py: URL et s√©lecteurs CSS")
    print("  - .env: Cl√©s API (GROQ_API_KEY)")


if __name__ == "__main__":
    asyncio.run(main())