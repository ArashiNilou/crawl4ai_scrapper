import asyncio
import time

from crawl4ai import AsyncWebCrawler
from dotenv import load_dotenv

from config import BASE_URL, REQUIRED_KEYS
from utils.data_utils import save_exposants_to_csv
from utils.scraper_utils import (
    get_browser_config,
    get_css_extraction_strategy,
    get_llm_strategy,
    extract_all_exposants,
    get_total_exposants_count
)

load_dotenv()


async def crawl_exposants_infinite_scroll():
    """
    Fonction principale pour crawler les exposants avec scroll infini.
    """
    print("=== D√©marrage du crawler d'exposants avec scroll infini ===")
    
    # Configuration
    browser_config = get_browser_config()
    
    # Strat√©gie d'extraction (CSS par d√©faut, LLM en fallback)
    css_strategy = get_css_extraction_strategy()
    llm_strategy = get_llm_strategy()
    
    session_id = "exposant_infinite_scroll_session"
    seen_names = set()
    
    start_time = time.time()
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        try:
            # √âtape 1: Obtenir une estimation du nombre total
            print("√âtape 1: Estimation du nombre total d'exposants...")
            total_estimate = await get_total_exposants_count(crawler, BASE_URL, session_id)
            print(f"Estimation: ~{total_estimate} exposants d√©tect√©s")
            
            # √âtape 2: Extraction avec CSS
            print("\n√âtape 2: Extraction avec strat√©gie CSS...")
            exposants_css = await extract_all_exposants(
                crawler, BASE_URL, session_id, css_strategy, 
                REQUIRED_KEYS, seen_names.copy()
            )
            
            # √âtape 3: Extraction avec LLM si CSS n'a pas bien fonctionn√©
            exposants_llm = []
            if len(exposants_css) < total_estimate * 0.5:  # Si moins de 50% r√©cup√©r√©s
                print("\n√âtape 3: Extraction avec strat√©gie LLM (fallback)...")
                exposants_llm = await extract_all_exposants(
                    crawler, BASE_URL, session_id, llm_strategy, 
                    REQUIRED_KEYS, seen_names.copy()
                )
            
            # √âtape 4: Combiner les r√©sultats
            print("\n√âtape 4: Combinaison des r√©sultats...")
            all_exposants = combine_results(exposants_css, exposants_llm, seen_names)
            
        except Exception as e:
            print(f"Erreur lors du crawling: {e}")
            return
    
    # √âtape 5: Sauvegarde
    end_time = time.time()
    duration = end_time - start_time
    
    if all_exposants:
        filename = f"exposants_scroll_{int(time.time())}.csv"
        save_exposants_to_csv(all_exposants, filename)
        
        print(f"\n=== R√âSULTATS ===")
        print(f"‚úÖ {len(all_exposants)} exposants extraits et sauvegard√©s")
        print(f"üìÅ Fichier: {filename}")
        print(f"‚è±Ô∏è  Dur√©e: {duration:.1f} secondes")
        print(f"üìä Taux de r√©ussite: {len(all_exposants)}/{total_estimate} ({len(all_exposants)/max(total_estimate,1)*100:.1f}%)")
        
        # Statistiques d√©taill√©es
        print(f"\n=== STATISTIQUES ===")
        secteurs = {}
        pays = {}
        for exp in all_exposants:
            secteur = exp.get('secteur_activite', 'N/A')
            secteurs[secteur] = secteurs.get(secteur, 0) + 1
            
            pays_exp = exp.get('pays', 'N/A') 
            pays[pays_exp] = pays.get(pays_exp, 0) + 1
        
        print(f"Top 5 secteurs:")
        for secteur, count in sorted(secteurs.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"  - {secteur}: {count}")
            
        print(f"Top 5 pays:")
        for pays_name, count in sorted(pays.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"  - {pays_name}: {count}")
            
    else:
        print("‚ùå Aucun exposant n'a √©t√© trouv√©")
        print("V√©rifiez:")
        print("1. L'URL dans config.py")
        print("2. Les s√©lecteurs CSS dans config.py")
        print("3. La structure HTML du site")
    
    # Afficher les statistiques LLM si utilis√©
    if 'llm_strategy' in locals():
        try:
            llm_strategy.show_usage()
        except:
            pass


def combine_results(css_results: list, llm_results: list, seen_names: set) -> list:
    """
    Combine les r√©sultats des deux strat√©gies d'extraction.
    """
    print(f"Combinaison: {len(css_results)} (CSS) + {len(llm_results)} (LLM)")
    
    # Commencer avec les r√©sultats CSS
    combined = css_results.copy()
    seen_names.update(exp.get('nom_entreprise', '') for exp in css_results)
    
    # Ajouter les r√©sultats LLM uniques
    added_from_llm = 0
    for exp in llm_results:
        name = exp.get('nom_entreprise', '')
        if name and name not in seen_names:
            combined.append(exp)
            seen_names.add(name)
            added_from_llm += 1
    
    print(f"R√©sultat final: {len(combined)} exposants ({added_from_llm} ajout√©s par LLM)")
    return combined


async def test_scroll_only():
    """
    Fonction de test pour v√©rifier l'acc√®s au site.
    Version simplifi√©e pour Crawl4AI 0.4.247
    """
    print("=== TEST: Acc√®s au site et analyse du contenu ===")
    
    browser_config = get_browser_config()
    session_id = "test_session"
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url=BASE_URL,
            config=CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                session_id=session_id,
            ),
        )
        
        if result.success and result.cleaned_html:
            html_content = result.cleaned_html
            print(f"‚úÖ Acc√®s r√©ussi, HTML r√©cup√©r√© ({len(html_content)} caract√®res)")
            
            # Sauvegarder pour inspection
            with open("debug_page.html", "w", encoding="utf-8") as f:
                f.write(html_content)
            print("üîç Page sauvegard√©e dans 'debug_page.html' pour inspection")
            
            # Analyser la structure
            import re
            
            # Chercher diff√©rents patterns d'√©l√©ments
            analysis = {
                "Divs avec class": len(re.findall(r'<div[^>]+class="[^"]*"', html_content)),
                "Spans avec class": len(re.findall(r'<span[^>]+class="[^"]*"', html_content)),
                "√âl√©ments avec 'company'": len(re.findall(r'class="[^"]*company[^"]*"', html_content, re.IGNORECASE)),
                "√âl√©ments avec 'card'": len(re.findall(r'class="[^"]*card[^"]*"', html_content, re.IGNORECASE)),
                "√âl√©ments avec 'exposant'": len(re.findall(r'class="[^"]*exposant[^"]*"', html_content, re.IGNORECASE)),
                "Attributs data-*": len(re.findall(r'data-[^=]+="[^"]*"', html_content)),
            }
            
            print("\nüìä Analyse de la structure HTML:")
            for element, count in analysis.items():
                if count > 0:
                    print(f"  - {element}: {count}")
            
            # Suggestions de s√©lecteurs
            print("\nüí° Suggestions pour am√©liorer les s√©lecteurs CSS:")
            print("1. Inspectez 'debug_page.html' dans un navigateur")
            print("2. Recherchez les patterns r√©currents autour des noms d'entreprises")
            print("3. Mettez √† jour CSS_SELECTORS dans config.py")
            
        else:
            print(f"‚ùå √âchec de l'acc√®s au site: {result.error_message if result else 'Pas de r√©ponse'}")
            print("V√©rifiez l'URL dans config.py")


async def main():
    """
    Point d'entr√©e principal.
    """
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        await test_scroll_only()
    else:
        await crawl_exposants_infinite_scroll()


if __name__ == "__main__":
    asyncio.run(main())