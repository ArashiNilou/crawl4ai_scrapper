# Configuration et utilisation du crawler scroll infini

## üîß √âtapes de configuration

### 1. Mise √† jour de config.py
```python
# Remplacez par votre vraie URL
BASE_URL = "https://votre-site-exposants.com"
```

### 2. Ajustement des s√©lecteurs CSS
Bas√© sur votre image, vous devez identifier les s√©lecteurs exacts. Voici comment proc√©der :

**M√©thode A: Inspection manuelle**
1. Ouvrez votre site dans Chrome/Firefox
2. Clic droit sur un nom d'entreprise ‚Üí "Inspecter l'√©l√©ment"
3. Notez le s√©lecteur CSS exact
4. R√©p√©tez pour chaque champ (secteur, pays, ville, etc.)

**M√©thode B: Test automatique des s√©lecteurs**
```bash
python main.py test
```
Cela va faire le scroll et compter les √©l√©ments d√©tect√©s.

### 3. Mise √† jour des s√©lecteurs dans config.py
```python
CSS_SELECTORS = {
    "container": "div.exposant-card",  # Conteneur de chaque exposant
    "nom_entreprise": "h2.company-name, h3.title",  # Nom de l'entreprise
    "secteur": "span.p-0.flex-1",  # Comme dans votre image
    "pays": ".text-sm span:nth-of-type(2)",  # √Ä ajuster
    "ville": "div:nth-of-type(3) p.text-xl",  # √Ä ajuster
    # ... autres s√©lecteurs
}
```

## üöÄ Utilisation

### Lancement standard
```bash
python main.py
```

### Test du scroll uniquement
```bash
python main.py test
```

### Avec debug d√©taill√©
Modifiez `headless=False` dans `get_browser_config()` pour voir le navigateur en action.

## üìã Processus du crawler

1. **Initialisation** : Ouvre le navigateur et charge la page
2. **Scroll infini** : Scroll automatique jusqu'√† charger tout le contenu
3. **Extraction CSS** : Utilise les s√©lecteurs CSS pour extraire rapidement
4. **Extraction LLM** : Fallback avec intelligence artificielle si n√©cessaire
5. **D√©duplication** : Supprime les doublons
6. **Sauvegarde** : Export en CSV avec timestamp

## üîç Diagnostic des probl√®mes

### Si aucun exposant n'est trouv√© :

1. **V√©rifiez l'URL**
   ```python
   BASE_URL = "https://votre-vraie-url.com"
   ```

2. **Testez les s√©lecteurs CSS**
   - Ouvrez la console du navigateur sur votre site
   - Testez : `document.querySelectorAll("votre-selecteur")`
   - Doit retourner les √©l√©ments attendus

3. **V√©rifiez la structure HTML**
   - Le contenu se charge-t-il vraiment par scroll ?
   - Y a-t-il des attributs `data-*` sp√©cifiques ?

4. **Ajustez le timing du scroll**
   ```python
   SCROLL_CONFIG = {
       "scroll_pause_time": 5,  # Plus lent si le site est lent
       "max_scrolls": 100,      # Plus de scrolls si beaucoup de contenu
   }
   ```

### Si le scroll ne fonctionne pas :

1. **V√©rifiez les triggers JavaScript**
   - Certains sites n√©cessitent des clics ou hover
   - Modifiez le script de scroll dans `scroll_and_load_content()`

2. **Testez avec headless=False**
   - Observez le comportement du scroll
   - V√©rifiez si de nouveaux √©l√©ments apparaissent

## üìä Optimisations possibles

### Pour de gros volumes (>1000 exposants) :
```python
# Dans config.py
SCROLL_CONFIG = {
    "scroll_pause_time": 1,  # Plus rapide
    "max_scrolls": 200,
    "scroll_height": 2000,   # Scrolls plus grands
}
```

### Pour des sites lents :
```python
SCROLL_CONFIG = {
    "scroll_pause_time": 5,  # Plus lent
    "no_new_content_limit": 5,  # Plus patient
}
```

## üêõ Debug avanc√©

### Logs d√©taill√©s
Tous les `print()` montrent le progression. Cherchez :
- "Scroll termin√©, contenu charg√©"
- "Donn√©es extraites brutes: X √©l√©ments" 
- "Exposant ajout√©: [nom]"

### Inspection du HTML
```python
# Ajoutez dans extract_all_exposants() apr√®s le scroll :
with open("debug_html.html", "w", encoding="utf-8") as f:
    f.write(result.cleaned_html)
```

### Test des s√©lecteurs un par un
```python
# Test individual des s√©lecteurs
test_selectors = {
    "nom": "h2, h3, .company-name",
    "secteur": "span.p-0.flex-1",
}

for name, selector in test_selectors.items():
    elements = await crawler.arun(
        url=BASE_URL,
        config=CrawlerRunConfig(
            css_selector=selector,
            cache_mode=CacheMode.BYPASS,
        )
    )
    print(f"{name}: {len(elements)} √©l√©ments trouv√©s")
```

## üìÑ Format de sortie

Le CSV contiendra :
- `nom_entreprise` : Nom de l'entreprise
- `secteur_activite` : Secteur principal  
- `pays` : Pays d'origine
- `ville` : Ville
- `emplacement` : Stand/emplacement
- `jours_presences` : Jours de pr√©sence
- `startup` : Statut startup (oui/non)
- `tags` : Tags/sous-secteurs
- `description` : Description de l'entreprise

## üîÑ Prochaines √©tapes

1. **Configurez votre URL** dans `config.py`
2. **Identifiez vos s√©lecteurs CSS** pr√©cis
3. **Testez avec** `python main.py test`
4. **Ajustez les s√©lecteurs** si n√©cessaire
5. **Lancez le crawl complet** avec `python main.py`

N'h√©sitez pas si vous avez des questions sur l'adaptation √† votre site sp√©cifique !